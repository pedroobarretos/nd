# -*- coding: utf-8 -*-
"""Trabalho2IA

# ALUNO: PEDRO HENRIQUE RODRIGUES BARRETO
# TREINAR UM CLASSIFICADOR DE TEXTO

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g8SG6sRfthBsiKS9Bl5qtPcmynPR9hU4
"""

!pip install transformers

import torch
import torch.nn as nn
from transformers import AutoTokenizer  # Or BertTokenizer
from transformers import AutoModelForPreTraining  # Or BertForPreTraining for loading pretraining heads
from transformers import AutoModel  # or BertModel, for BERT without pretraining heads
import numpy as np

from google.colab import drive #conectando o drive
drive.mount('/content/drive')

import pandas as pd
import os

os.chdir('/content/drive/MyDrive/TextosIA') #acesso à pasta que contem os arquivos de texto

!ls

with open('FrasesAmor.txt', 'r', encoding='utf-8') as file:
    texto_amor = file.read()

# Divide o texto
frases_amor = texto_amor.replace('\n', ' ').split('. ')
frases_amor = [f.strip() for f in frases_amor if f]  # Remove espaços extras

# Cria DataFrame para as frases de amor
data_amor = pd.DataFrame(frases_amor, columns=['text'])

# Exibe as primeiras linhas
print(data_amor.head())

# Ler o arquivo de textos tristes
with open('FrasesTristeza.txt', 'r', encoding='utf-8') as file:
    texto_tristeza = file.read()

# Divide o texto usando delimitadores
frases_tristeza = texto_tristeza.replace('\n', ' ').split('. ')
frases_tristeza = [f.strip() for f in frases_tristeza if f]  # Remove espaços extras

# Cria DataFrame para as frases de tristeza
data_tristeza = pd.DataFrame(frases_tristeza, columns=['text'])

# Exibe as primeiras linhas
print(data_tristeza.head())

# Salvar como CSVs organizado, se necessário
data_amor.to_csv('amor_processado.csv', index=False)
data_tristeza.to_csv('tristeza_processado.csv', index=False)

from transformers import BertTokenizer, BertModel
tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')
model = BertModel.from_pretrained("bert-base-multilingual-cased")

model

#model = AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')
#tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased', do_lower_case=False)

meu_modelo = model

meu_modelo

model.config

from transformers import BertPreTrainedModel
from typing import List, Optional, Tuple, Union

class LIABertClassifier(nn.Module):
    def __init__(self,model,num_labels):
        super(LIABertClassifier,self).__init__()
        self.bert = model
        self.config = model.config
        self.num_labels = num_labels
        self.cls = nn.Linear(self.config.hidden_size,400)
        self.dropout = nn.Dropout(p=0.5)
        self.cls2 = nn.Linear(400,num_labels)
        self.gelu = nn.GELU()
        self.join_strategy = 'zero'

    def forward(
        self,
        input_ids: Optional[torch.Tensor] = None,
        attention_mask: Optional[torch.Tensor] = None,
        token_type_ids: Optional[torch.Tensor] = None,
        ) ->Tuple[torch.Tensor]:

        outputs = self.bert(
            input_ids,
            attention_mask=attention_mask,
            token_type_ids=token_type_ids,
        )
        if self.join_strategy == 'zero':
            sequence_output = outputs[0][:,0,:]
        else:
            last_layer= outputs[0][:,:,:]
            sequence_output = torch.mean(last_layer,dim=1)
        prediction = self.cls(sequence_output)
        prediction = self.gelu(prediction)
        prediction = self.dropout(prediction)
        prediction = self.cls2(prediction)
        return prediction

model = LIABertClassifier(model=meu_modelo,num_labels=2)

model

tokenizer.tokenize("Estou fazendo o trabalho.") #testando o tokenizador

token=tokenizer("Estou fazendo o trabalho.", return_tensors='pt')

token

import sklearn.model_selection as model_selection

xtrain_data_amor = np.array(data_amor['text'])

ytrain_data_amor = np.array(data_amor['text'])

xtrain, yval = data_amor['text'], data_amor['text']

ytrain, xval = data_tristeza['text'], data_tristeza['text']

for v in zip(xtrain[:3],ytrain[:3]):
    print(v)

train_encodings = tokenizer(xtrain, truncation=True, padding=True, max_length=512, return_tensors='pt')
val_encodings = tokenizer(xval.tolist(), truncation=True, padding=True, max_length=512, return_tensors='pt')

class MyDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: val[idx] for key, val in self.encodings.items()}
        label = torch.tensor(self.labels[idx])
        return (item,label)

    def __len__(self):
        return len(self.labels)

ret=train_encodings.items()

ds_train = MyDataset(train_encodings,ytrain)
ds_val   = MyDataset(val_encodings,yval)

np.unique(ytrain,return_counts=True)

from torch.utils.data import DataLoader

dl_train = DataLoader(ds_train,batch_size=8)
dl_eval  = DataLoader(ds_val,batch_size=8)

x,y = next(iter(dl_train))

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

device

batch = {k: v.to(device) for k, v in x.items()}

from transformers import get_scheduler

from torch.optim import AdamW
optimizer = AdamW(model.parameters(), lr=1e-5)

num_epochs = 10

loss_fct = nn.CrossEntropyLoss()

for epoch in range(num_epochs):
    lepochs = []
    for batch,y in dl_train:
        batch = {k: v.to(device) for k, v in batch.items()}
        y     = y.to(device)
        outputs = model(**batch)
        loss = loss_fct(outputs,y)
        loss.backward()
        lepochs.append(loss.cpu().item())
        optimizer.step()
        #lr_scheduler.step()
        optimizer.zero_grad()
    print(np.mean(lepochs))

model.eval()

ytrue = []
ypred = []
for batch,y in dl_eval:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)
    predictions = torch.argmax(outputs, dim=-1)
    ytrue += y.tolist()
    ypred += predictions.cpu().tolist()

x,y = next(iter(dl_eval))

with torch.no_grad():
    outputs = model(**batch)

model

"""#código para salvar e carregar o modelo"""

torch.save(model.state_dict(),'/content/drive/MyDrive/model.pth')

backup = torch.load('/content/drive/MyDrive/model.pth')

model.load_state_dict(backup)



"""# Avaliação do modelo"""

from sklearn import metrics

metrics.confusion_matrix(ytrue,ypred)

print(metrics.classification_report(ytrue,ypred))

"""precision    recall  f1-score   support

           0       0.67      0.77      0.72       151
           1       0.77      0.66      0.71       169

    accuracy                           0.71       320
   macro avg       0.72      0.72      0.71       320
weighted avg       0.72      0.71      0.71       320

# Exemplo de classificação de texto
"""

token=tokenizer(["good food"],return_tensors='pt')

out=model(input_ids=token['input_ids'].to(device))

out

if torch.argmax(out, dim=-1)[0].cpu().item() == 0:
    print('negativo')
else:
    print('positivo')

